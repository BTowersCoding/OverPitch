\documentclass[letterpaper]{article}
\usepackage{natbib,alifexi}
\usepackage[inline]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{cases}
\usepackage{amsfonts}

\title{Pitch Scaling of Music Signals}
\author{Théo Verhelst \\
Université Libre de Bruxelles}


\begin{document}
\maketitle

\begin{abstract}
Pitch scaling is the task of modifying the frequency of a signal while keeping
its playback speed intact. Pitch scaling is an important feature of many digital
music tools, and needs to be done in real-time with the highest available
quality. In this report, we describe succintly state-of-the-art techniques of
pitch scaling, and we explain in detail one that is particularly suited to music
signals. We also describe a Clojure implementation of this technique. This
technique uses phase vocoder for time-scaling and 3rd order spline interpolation
for resampling.
\end{abstract}

\section{Introduction}
Pitch scaling is a process that changes the frequency of a signal without
modifying its speed. The main difficulty of pitch scaling is to make the
synthetised signal as natural as the original, so that it sounds like as if it
was recorded or created on this new pitch. More precisely, the timbre and the
speed of the signal need to be preserved.
\paragraph{}
One application of pitch scaling is to scale the pitch of an instrument
recording in a music production software, in order to tune it to the other
instruments, or for any other musical purpose. But the main use of pitch scaling
is probably in DJ software: one can change the pitch of a track, and therefore
its key, in order to make the transition to the next track easier and smoother.
\paragraph{}
The most straightforward way to change the pitch of an audio signal is to
resample the signal and playing it back at its original rate \citep{DM}. But
both pitch and speed are modified at the same time. Thus, we need a more
sofisticated technique in order to preserve the playback speed constant.
\paragraph{}
In contrast to pitch scaling, time-scale modification (TSM) is a process that
modifies the speed of a signal without modifying its pitch and its timbre.
TSM has been subject to many more studies than pitch scaling, but we base our
work on these studies, since it is possible to show that both processes are
mathematically equivalent. Indeed, in order to change the pitch of a signal, one
can use a well-known TSM method, and then resample the signal.

\section{Time-scale modification techniques}
TSM techniques can be grouped in two main categories: \emph{time-domain TSM} and
\emph{frequency-domain TSM}.
\paragraph{}
Time-domain TSM extracts portions of the input signal at a frequency defined by
the scaling factor, and places them in the output signal. This technique is
particularly suited to monophonic, harmonic signal, as it preserves almost
perfectly the timbre of the signal. The basic time-domain algorithm,
\emph{overlap-and-add} (or \emph{OLA}), suffers from phase jumps artifacts, but there are many
variations of this algorithm that avoid this effect. But all of them are only
able to correct phase jumps on the most prominent frequency, i.e. the fundamental
frequency of the signal. Phase jumps can still occur in less important frequencies,
i.e. the harmonics, which is clearly audible. OLA-base algorithms are thus not
suited to polyphonic signals, since they contains more harmonics than monophonic
ones. Furthermore, non-harmonic signals, such as drums or percussive instruments, have non-periodic patterns. These patterns
are known as \emph{transients}. A typical time-domain TSM technique leads to
transient doubling or skipping (depending on the scaling factor), since these
techniques periodically repeat or discard some small parts of the signal. This
can be avoided by taking a very short frame size.
\paragraph{}
Frequency-domain TSM is based on the short-time Fourier transform (STFT). It
splits the signal in small chunks, and computes the Fourier transform of each
of these chunks, in order to get a discrete frequency-domain representation of the
signal. Often, the technique also uses the \emph{phase vocoder} in order to
refine the frequencies estimates, and are thus named \emph{phase-vocoder
time-scale modification}, or \emph{PV-TSM}, or simply phase vocoder. The idea is
to preserve phase continuity across all frequencies, and not only on the most
prominent frequency as in time-domain TSM, by exploiting the frequency-domain
representation of the sound. PV-TSM behaves well on polyphonic signals, but are
subject to vertical phase incoherence, i.e. the relationship between the phases
of different frequencies at a point of time is not preserved, leading to audible
artifacts, known as \emph{phasiness}, or \emph{loss of presence}.

\section{General procedure}
Pitch scaling process is split in two steps: \begin{enumerate*}[label=\arabic*)]
\item apply a TSM procedure \item resample the signal\end{enumerate*}. Let
\(\alpha\) be the scaling factor. We first apply a TSM procedure with parameter
\(\alpha\), so that the playback speed is modified, while the pitch is left
unmodified. Then, we resample the signal by a factor \(1/\alpha\), so that the
playback speed of the signal is the same as the original, but the pitch is
multiplied by \(\alpha\).
\paragraph{}
For the TSM part, we use a simple phase vocoder with phase propagation. For the
resampling part, we use an interpolator based on 3rd order splines. These are
explained below.

\section{Time-scale modification}
\subsection{Basics of time-scale modification}
\paragraph{Notation:} Since range of integers are often used in this algorithm,
let  \([a:b]:=\{a,a+1,\dots,b-1,b\}\) and \([a:b[:=\{a,a+1,\dots,b-1\}\)
\(\forall a,b\in\mathbb{Z}:a<b\).
\paragraph{}
First, we have to define the basic concepts involved in time-scaling. Let the
function \(x:\mathbb{Z}\to[-1, 1]\) be signal to time-scale. In practice,
the analysed audio signal has a finite duration of \(L\in\mathbb{N}\) samples.
Thus we define \(x(n)=0\,\forall n\in\mathbb{Z}\setminus [0:L[ \) for the sake
of simplicity. We want to construct the output signal
\(y:\mathbb{Z}\to[-1, 1]\) that have the same frequency-domain properties as
\(x\), but being stretched in time by the factor \(\alpha\).
\paragraph{}
Almost all TSM techniques are based on the following procedure: first, \(x\)
is divised in \emph{analysis frames} \(x_m,\,m\in\mathbb{Z}\) having each a
length of \(N\) samples, and these analysis frames are spaced by an
\emph{analysis hopsize} \(H_a\):
\[x_m(n)=\begin{cases}
	x(mH_a + n) & \text{if }n\in [-N/2:N/2[ \\
	0           & \text{otherwise}
\end{cases}\]
Then, we could want to put these frames in the output signal by spacing them by
the synthesis hopsize \(H_s\), achieving the correct time-scale modification. But this
would cause very audible phase discontinuities, since the end of a frame would
no longer match with the beginning of the next frame. Thus, we need to modify
the analysis frames \(x_m\) into \emph{synthesis frames} \(y_m\) before adding
them to the output signal \(y(n)\):
\[y(n) = \sum_{m\in\mathbb{Z}}y_m(n-mH_s)\]
The method used to transform \(x_m\) into \(y_m\) is critical, as it determines
the quality of the result. In addition to phase discontinuities, it must also
compensate gain fluctuations: if no care is taken, two overlaping frames may have
a higher gain in the overlaping part than in the rest of the signal.
\begin{figure}[h]
\includegraphics[width=\linewidth]{pipeline.png}
\caption{General procedure of time-scale modification.}
\end{figure}
\paragraph{}
\(H_s\) is usually set to \(N/2\) or \(N/4\), in order to have a constant
overlap between the synthesis frames. And since we know that
\(\alpha=\frac{H_s}{H_a}\), we then have \(H_a=\frac{H_s}{\alpha}\).
\paragraph{}
A common procedure to compensate gain fluctuations is to multiply each modified
frame by a windowing function \(w:\mathbb{N}\to[0,1]\) before adding them to the
output signal \(y\). The Hann window is widely used for this purpose, and is
defined as
\[w(n)=\begin{cases}
  0.5(1-cos(\frac{2\pi (n + N/2)}{N-1}))\text{ if }n\in[-N/2:N/2[\\
  0 \text{ otherwise}\end{cases}\]
\begin{figure}[h]
\includegraphics[width=\linewidth]{hann.png}
\caption{The Hann window function.}
\end{figure}

\subsection{Phase-vocoder time-scale modification}
The phase vocoder is a common frequency-domain technique used to derive the
\emph{synthesis frames} \(y_m\)  from the analysis frames \(x_m\). The idea
is to compute the spectrum of the frame with the Fourier transform, and change
the phases of this spectrum so that the signal resynthetised from this modified
spectrum has no phase jump with the following and previous frames.
\paragraph{}The spectrum of a signal is the frequency-domain representation of
a time-domain signal, and is composed by the amplitudes and phases of the
Fourier series decomposition of the signal. More precisely, the Fourier
transform of the frame \(x_m\) is given by
\[X(m,k) = \sum_{n=-N/2}^{N/2-1}x(n)w(n)\exp(-2\pi ikn/N)\]


\subsection{Result and limitations}
\subsection{Further improvements}

\section{Resampling}
The goal of resampling is to reconstruct a continuous-time signal from the given
discrete-time samples, and then sample this signal again with another sampling
rate. More formally, we want to construct the continuous-time signal
\[\hat x:\mathbb{R}\to\mathbb{R}\]
such that
\[x(n) = \hat x(Tn) \;\forall n\in\mathbb{Z}\]
where \(T\) is the sampling period (the inverse of the sampling rate). Then, we
sample a new signal \(y\) at a sampling period \(T'\):
\[y(n) = \hat x(T'n) \;\forall n\in\mathbb{Z}\]
In practice, we often search a direct relationship between \(x\) and \(y\),
without expressing \(\hat x\) directly.
\paragraph{}
In order to construct the continuous-time signal, we need an interpolator. There
exists various interpolators, such as truncated sinc, linear-interpolator,
b-spline interpolator, Lagrange interpolator. We chose 3rd order spline
interpolator, for its simple implementation, although it gives not the best
results for musical signal interpolation.
\paragraph{}
For 3rd order spline interpolation, we search the factors of a 3rd-degree
polynomial for each pair of consecutive signal values \((x(n),\,x(n+1))\), such
that this polynomial passes through these values. For the sake of the notation,
let
\[y_0=x(n-1);\;y_1=x(n);\;y_2=x(n+1);\;y_3=x(n+2)\]
We search a function
\begin{equation}
\label{interpolation_poly}
f:[0, 1]\to\mathbb{R}:t\mapsto \sum_{i=0}^{3}\alpha_i t^i
=\alpha_0+\alpha_1 t+\alpha_2t^2+\alpha_3t^3
\end{equation}
In order to determine the value of the factors \(\alpha_i\), we need to set four
constraints on the function \(f\):
\begin{numcases}{ }
f(0)=y_1\label{constr1}\\
f(1)=y_2\label{constr2}\\
f'(0)=\frac{y_2-y_0}{2}\label{constr3} \\
f'(1)=\frac{y_3-y_1}{2}\label{constr4}
\end{numcases}
\eqref{constr1} and \eqref{constr2} are natural constraints of spline
interpolator: we want that the interpolating function passes through the
supplied values
\((x(n),\,x(n+1))\). But in order to determine the factors \(\alpha_i\), we need
two more constraints. One solution is to use Hermitian splines, that is,
the derivative of \(f\) at \(t\in\{0, 1\}\) is equal to the derivative of a
straight line between the previous and the next point (\eqref{constr3} and
\eqref{constr4}). By expressing the equations as a matrix equation,
we can find that
\begin{align*}
\begin{bmatrix}
	\alpha_0\\\alpha_1\\\alpha_2\\\alpha_3
\end{bmatrix}
&=
\begin{bmatrix}
	 1 &  0 &   0 &  0   \\
	 0 &  0 & 0.5 &  0   \\
	-3 &  3 &  -1 & -0.5 \\
	 2 & -2 & 0.5 &  0.5 \\
\end{bmatrix}
\begin{bmatrix}
	y_1\\y_2\\y_2-y_0\\y_3-y_1
\end{bmatrix}
\end{align*}
For each pair of samples, we just have to compute the four factors \(\alpha_i\)
with this equation, and then evaluate the function \(f\) as in
\eqref{interpolation_poly}, at a value of \(t\) corresponding to the new
sampling rate.
\section{Clojure implementation}
\subsection{The Clojure programming language}
\subsection{Overtone}
\subsection{Implementation of pitch scaling}

\section{Results}
\section{Conclusion}

\footnotesize
\bibliographystyle{apalike}
\bibliography{Report}


\end{document}
